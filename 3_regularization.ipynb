{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_regularization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/vmartinezalvarez/Deep-Learning-by-Google_UDACITY/blob/master/3_regularization.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "kR-4eNdK6lYS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Deep Learning\n",
        "=============\n",
        "\n",
        "Assignment 3\n",
        "------------\n",
        "\n",
        "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
        "\n",
        "The goal of this assignment is to explore regularization techniques."
      ]
    },
    {
      "metadata": {
        "id": "JLpLa8Jt7Vu4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pickle\n",
        "from hashlib import sha1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1HrCK6e17WzV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First reload the data we generated in `1_notmnist.ipynb`."
      ]
    },
    {
      "metadata": {
        "id": "y3-cj1bpmuxc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "95f0b2a3-dea0-45a3-ab4e-d5492e7563ae"
      },
      "cell_type": "code",
      "source": [
        "pickle_file = 'notMNIST.pickle'\n",
        "\n",
        "with open(pickle_file, 'rb') as f:\n",
        "  save = pickle.load(f)\n",
        "  train_dataset1 = save['train_dataset']\n",
        "  train_labels1 = save['train_labels']\n",
        "  valid_dataset1 = save['valid_dataset']\n",
        "  valid_labels1 = save['valid_labels']\n",
        "  test_dataset1 = save['test_dataset']\n",
        "  test_labels1 = save['test_labels']\n",
        "  del save  # hint to help gc free up memory\n",
        "  print('Training set', train_dataset1.shape, train_labels1.shape)\n",
        "  print('Validation set', valid_dataset1.shape, valid_labels1.shape)\n",
        "  print('Test set', test_dataset1.shape, test_labels1.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 28, 28) (200000,)\n",
            "Validation set (10000, 28, 28) (10000,)\n",
            "Test set (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L7aHrm6nGDMB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reformat into a shape that's more adapted to the models we're going to train:\n",
        "- data as a flat matrix,\n",
        "- labels as float 1-hot encodings."
      ]
    },
    {
      "metadata": {
        "id": "IRSyYiIIGIzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "13d3cb74-4e7b-4a09-f6a3-3d3e1c98dd74"
      },
      "cell_type": "code",
      "source": [
        "image_size = 28\n",
        "num_labels = 10\n",
        "\n",
        "def reformat(dataset, labels):\n",
        "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
        "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
        "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
        "  return dataset, labels\n",
        "train_dataset, train_labels = reformat(train_dataset1, train_labels1)\n",
        "valid_dataset, valid_labels = reformat(valid_dataset1, valid_labels1)\n",
        "test_dataset, test_labels = reformat(test_dataset1, test_labels1)\n",
        "print('Training set', train_dataset.shape, train_labels.shape)\n",
        "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
        "print('Test set', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (200000, 784) (200000, 10)\n",
            "Validation set (10000, 784) (10000, 10)\n",
            "Test set (10000, 784) (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RajPLaL_ZW6w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgLbUAQ1CW-1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 1\n",
        "---------\n",
        "\n",
        "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "FjeVdNBcdovQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logistic Classifier Sklearn"
      ]
    },
    {
      "metadata": {
        "id": "PIoFrUvJSFPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "10dbb7c9-de7b-4218-ad42-10131e2c89f0"
      },
      "cell_type": "code",
      "source": [
        "a= train_dataset[:10000,:]\n",
        "print(a.shape)\n",
        "b = train_labels[:10000]\n",
        "train_dataset.shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "oDg4fVQX0GNA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b64d7bb-e23e-4fd0-b2d4-ddd8df57e8ca"
      },
      "cell_type": "code",
      "source": [
        "from scipy import ndimage\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "logit_clf = LogisticRegression(penalty='l2')\n",
        "\n",
        "logit_clf.fit(train_dataset[:10000,:], train_labels1[:10000])\n",
        "\n",
        "predicted = logit_clf.predict(test_dataset)\n",
        "print ('accuracy', accuracy((np.arange(num_labels) == predicted[:,None]).astype(np.float32), test_labels), '%')\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 86.02 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZET5YpVsd55a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shallow Multinomial Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "3dFEj9PwgNDt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "b704d9e9-a625-433a-829f-66ae572e85b9"
      },
      "cell_type": "code",
      "source": [
        "# With gradient descent training, even this much data is prohibitive.\n",
        "# Subset the training data for faster turnaround.\n",
        "train_subset = 10000\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data.\n",
        "  # Load the training, validation and test data into constants that are\n",
        "  # attached to the graph.\n",
        "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
        "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  # Variables.\n",
        "  # These are the parameters that we are going to be training. The weight\n",
        "  # matrix will be initialized using random values following a (truncated)\n",
        "  # normal distribution. The biases get initialized to zero.\n",
        "  weights = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "  biases = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
        "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
        "  # it's very common, and it can be optimized). We take the average of this\n",
        "  # cross-entropy across all training examples: that's our loss.\n",
        "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
        "  \n",
        "  l2_regularizer = tf.nn.l2_loss(weights)\n",
        "  loss += 5e-4 * l2_regularizer\n",
        "  \n",
        "  \n",
        "  # Optimizer.\n",
        "  # We are going to find the minimum of this loss using gradient descent.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  # These are not part of training, but merely here so that we can report\n",
        "  # accuracy figures as we train.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  valid_prediction = tf.nn.softmax(\n",
        "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-35-b29b3ab7018b>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LXdZ_3R4gM-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "23d0a48f-8c2b-40c2-c935-cd5f126c81ba"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 801\n",
        "\n",
        "def accuracy(predictions, labels):\n",
        "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
        "          / predictions.shape[0])\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  # This is a one-time operation which ensures the parameters get initialized as\n",
        "  # we described in the graph: random weights for the matrix, zeros for the\n",
        "  # biases. \n",
        "  tf.global_variables_initializer().run()\n",
        "  print('Initialized')\n",
        "  for step in range(num_steps):\n",
        "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
        "    # and get the loss value and the training predictions returned as numpy\n",
        "    # arrays.\n",
        "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
        "    if (step % 100 == 0):\n",
        "      print('Loss at step %d: %f' % (step, l))\n",
        "      print('Training accuracy: %.1f%%' % accuracy(\n",
        "        predictions, train_labels[:train_subset, :]))\n",
        "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
        "      # just to get that one numpy array. Note that it recomputes all its graph\n",
        "      # dependencies.\n",
        "      print('Validation accuracy: %.1f%%' % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Loss at step 0: 16.731560\n",
            "Training accuracy: 16.8%\n",
            "Validation accuracy: 19.8%\n",
            "Loss at step 100: 3.533545\n",
            "Training accuracy: 73.1%\n",
            "Validation accuracy: 71.6%\n",
            "Loss at step 200: 2.960108\n",
            "Training accuracy: 76.1%\n",
            "Validation accuracy: 74.1%\n",
            "Loss at step 300: 2.609129\n",
            "Training accuracy: 77.4%\n",
            "Validation accuracy: 75.1%\n",
            "Loss at step 400: 2.352725\n",
            "Training accuracy: 78.3%\n",
            "Validation accuracy: 75.4%\n",
            "Loss at step 500: 2.148804\n",
            "Training accuracy: 79.0%\n",
            "Validation accuracy: 75.6%\n",
            "Loss at step 600: 1.979333\n",
            "Training accuracy: 79.6%\n",
            "Validation accuracy: 75.7%\n",
            "Loss at step 700: 1.835155\n",
            "Training accuracy: 80.3%\n",
            "Validation accuracy: 75.9%\n",
            "Loss at step 800: 1.710586\n",
            "Training accuracy: 80.8%\n",
            "Validation accuracy: 76.2%\n",
            "Test accuracy: 83.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HTmaTlW0hKd_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stochastic Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "nbhndftreARo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  # Variables.\n",
        "  weights = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
        "  biases = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  # Training computation.\n",
        "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
        "  \n",
        "  l2_regularizer = tf.nn.l2_loss(weights)\n",
        "  loss += 10e-4 * l2_regularizer\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "   \n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  valid_prediction = tf.nn.softmax(\n",
        "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
        "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2FyChvYoeByj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "b8763495-eda0-4011-9cb2-38cab34c4e5e"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 22.273397\n",
            "Minibatch accuracy: 8.6%\n",
            "Validation accuracy: 13.6%\n",
            "Minibatch loss at step 500: 3.287861\n",
            "Minibatch accuracy: 71.9%\n",
            "Validation accuracy: 76.4%\n",
            "Minibatch loss at step 1000: 1.928576\n",
            "Minibatch accuracy: 74.2%\n",
            "Validation accuracy: 78.1%\n",
            "Minibatch loss at step 1500: 1.214303\n",
            "Minibatch accuracy: 82.0%\n",
            "Validation accuracy: 80.2%\n",
            "Minibatch loss at step 2000: 0.978054\n",
            "Minibatch accuracy: 82.0%\n",
            "Validation accuracy: 81.5%\n",
            "Minibatch loss at step 2500: 0.725850\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 81.8%\n",
            "Minibatch loss at step 3000: 0.731505\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 82.2%\n",
            "Test accuracy: 88.2%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "17z_jCB4iiMT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Logistic Regression  with SGD, 1-hidden layer neural network with rectified linear unit"
      ]
    },
    {
      "metadata": {
        "id": "5YRyVBP8igww",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "hidden_nodes = 1024\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
        "  biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
        "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "\n",
        "  hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  \n",
        "  \n",
        "  # Training computation.\n",
        "  logits = tf.matmul(hidden1, weights2) + biases2\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
        "  \n",
        "  l2_regularizer = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)\n",
        "  loss += 10e-4 * l2_regularizer\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  \n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  valid_prediction = tf.nn.softmax(\n",
        "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
        "  test_prediction = tf.nn.softmax(\n",
        "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ssiGtkSigkN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "ea08c83b-6ef2-44ee-a144-826c42d44212"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 602.629822\n",
            "Minibatch accuracy: 12.5%\n",
            "Validation accuracy: 35.0%\n",
            "Minibatch loss at step 500: 197.144058\n",
            "Minibatch accuracy: 77.3%\n",
            "Validation accuracy: 80.0%\n",
            "Minibatch loss at step 1000: 114.184151\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 82.6%\n",
            "Minibatch loss at step 1500: 68.517105\n",
            "Minibatch accuracy: 86.7%\n",
            "Validation accuracy: 83.9%\n",
            "Minibatch loss at step 2000: 41.573666\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 84.7%\n",
            "Minibatch loss at step 2500: 25.087870\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 86.8%\n",
            "Minibatch loss at step 3000: 15.513335\n",
            "Minibatch accuracy: 88.3%\n",
            "Validation accuracy: 87.2%\n",
            "Test accuracy: 92.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "na8xX2yHZzNF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 2\n",
        "---------\n",
        "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "-LOXpW1M0HG3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "hidden_nodes = 1024\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32,\n",
        "                                    shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  # Variables.\n",
        "  weights1 = tf.Variable(\n",
        "    tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
        "  biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
        "  weights2 = tf.Variable(\n",
        "    tf.truncated_normal([hidden_nodes, num_labels]))\n",
        "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "\n",
        "  hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
        "  \n",
        "  \n",
        "  # Training computation.\n",
        "  logits = tf.matmul(hidden1, weights2) + biases2\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
        "  \n",
        "  l2_regularizer = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)\n",
        "  loss += 10e-4 * l2_regularizer\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
        "  \n",
        "  # Predictions for the training, validation, and test data.\n",
        "  \n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  valid_prediction = tf.nn.softmax(\n",
        "    tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1), weights2) + biases2)\n",
        "  test_prediction = tf.nn.softmax(\n",
        "    tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1), weights2) + biases2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KCreLd2X0G2c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "5ee18a7c-cf36-44fa-ee62-edcb61661e18"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 3001\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
        "        valid_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 705.427368\n",
            "Minibatch accuracy: 0.0%\n",
            "Validation accuracy: 11.6%\n",
            "Minibatch loss at step 500: 150239.437500\n",
            "Minibatch accuracy: 50.0%\n",
            "Validation accuracy: 44.9%\n",
            "Minibatch loss at step 1000: 69008.484375\n",
            "Minibatch accuracy: 60.0%\n",
            "Validation accuracy: 53.7%\n",
            "Minibatch loss at step 1500: 200728.703125\n",
            "Minibatch accuracy: 70.0%\n",
            "Validation accuracy: 61.3%\n",
            "Minibatch loss at step 2000: 201098.250000\n",
            "Minibatch accuracy: 40.0%\n",
            "Validation accuracy: 50.7%\n",
            "Minibatch loss at step 2500: 415705.500000\n",
            "Minibatch accuracy: 50.0%\n",
            "Validation accuracy: 48.9%\n",
            "Minibatch loss at step 3000: 94442.453125\n",
            "Minibatch accuracy: 60.0%\n",
            "Validation accuracy: 51.7%\n",
            "Test accuracy: 55.7%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ww3SCBUdlkRc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 3\n",
        "---------\n",
        "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
        "\n",
        "What happens to our extreme overfitting case?\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "5tLT8xd60Ll-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-b1hTz3VWZjw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Problem 4\n",
        "---------\n",
        "\n",
        "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
        "\n",
        "One avenue you can explore is to add multiple layers.\n",
        "\n",
        "Another one is to use learning rate decay:\n",
        "\n",
        "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
        "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        " \n",
        " ---\n"
      ]
    },
    {
      "metadata": {
        "id": "F1cU_mianLLh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "19da875b-028f-445e-eed2-c10425e30d59"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def get_dataset_hash(dataset):\n",
        "  return [\n",
        "          sha1(dataset_instance).digest() \n",
        "          for dataset_instance in dataset\n",
        "         ]\n",
        "\n",
        "train_dataset_hash = get_dataset_hash(train_dataset)\n",
        "valid_dataset_hash = get_dataset_hash(valid_dataset)\n",
        "test_dataset_hash = get_dataset_hash(test_dataset)\n",
        "\n",
        "duplicates_in_train_and_valid_dataset = np.intersect1d(train_dataset_hash, valid_dataset_hash)\n",
        "duplicates_in_train_and_test_dataset = np.intersect1d(train_dataset_hash, test_dataset_hash)\n",
        "duplicates_in_valid_and_test_dataset = np.intersect1d(test_dataset_hash, valid_dataset_hash)\n",
        "\n",
        "duplicates = np.hstack(\n",
        "                       (duplicates_in_train_and_valid_dataset,\n",
        "                        duplicates_in_train_and_test_dataset,\n",
        "                        duplicates_in_valid_and_test_dataset)\n",
        "                      )\n",
        "\n",
        "def get_sanitize(dataset, dataset_hash, exclude_hash):\n",
        "  return np.array([\n",
        "                   dataset[index] for index in \n",
        "                   np.arange(dataset.shape[0]) if\n",
        "                   dataset_hash[index] not in \n",
        "                   exclude_hash\n",
        "                 ])\n",
        "\n",
        "sanitized_valid_dataset = get_sanitize(valid_dataset, valid_dataset_hash, duplicates)\n",
        "sanitized_test_dataset = get_sanitize(test_dataset, test_dataset_hash, duplicates)\n",
        "\n",
        "print ('original valid dataset shape',  valid_dataset.shape)\n",
        "print ('sanitized valid dataset shape', sanitized_valid_dataset.shape)\n",
        "print ('original test dataset shape',  test_dataset.shape)\n",
        "print ('sanitized test dataset shape', sanitized_test_dataset.shape)\n",
        "\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original valid dataset shape (10000, 784)\n",
            "sanitized valid dataset shape (8900, 784)\n",
            "original test dataset shape (10000, 784)\n",
            "sanitized test dataset shape (8723, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j_egCEv6nLHM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "hidden_nodes_1 = 1024\n",
        "hidden_nodes_2 = 512\n",
        "hidden_nodes_3 = 128\n",
        "#hidden_nodes_4 = 64\n",
        "\n",
        "hidden_stddev_1 = np.sqrt(2.0/784) \n",
        "hidden_stddev_2 = np.sqrt(2.0/hidden_nodes_1)\n",
        "hidden_stddev_3 = np.sqrt(2.0/hidden_nodes_2)\n",
        "output_stddev   = np.sqrt(2.0/hidden_nodes_3)\n",
        "\n",
        "\n",
        "regu = 10e-4 \n",
        "\n",
        "keep_prob_1 = 0.5\n",
        "keep_prob_2 = 0.6\n",
        "keep_prob_3 = 0.7\n",
        "\n",
        "deep_graph = tf.Graph()\n",
        "with deep_graph.as_default():\n",
        "\n",
        "  # Input data. For the training data, we use a placeholder that will be fed\n",
        "  # at run time with a training minibatch.\n",
        "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
        "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
        "  tf_valid_dataset = tf.constant(valid_dataset)\n",
        "  tf_test_dataset = tf.constant(test_dataset)\n",
        "  \n",
        "  \n",
        "  # first hidden layer\n",
        "  hidden_weights_1 = tf.Variable(\n",
        "    tf.truncated_normal(\n",
        "      [image_size * image_size, hidden_nodes_1], stddev=hidden_stddev_1)) #\n",
        "  hidden_biases_1 = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
        "  hidden_1 = tf.nn.dropout(\n",
        "    tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights_1) + hidden_biases_1), keep_prob_1)\n",
        "  \n",
        "  # second hidden layer\n",
        "  hidden_weights_2 = tf.Variable(\n",
        "    tf.truncated_normal(\n",
        "      [hidden_nodes_1, hidden_nodes_2], stddev=hidden_stddev_2)) #\n",
        "  hidden_biases_2 = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
        "  hidden_2 = tf.nn.dropout(\n",
        "    tf.nn.relu(tf.matmul(hidden_1, hidden_weights_2) + hidden_biases_2), keep_prob_2)\n",
        "  \n",
        "  # third hidden layer\n",
        "  hidden_weights_3 = tf.Variable(\n",
        "    tf.truncated_normal(\n",
        "      [hidden_nodes_2, hidden_nodes_3], stddev=hidden_stddev_3)) #\n",
        "  hidden_biases_3 = tf.Variable(tf.zeros([hidden_nodes_3]))\n",
        "  hidden_3 = tf.nn.dropout(\n",
        "    tf.nn.relu(tf.matmul(hidden_2, hidden_weights_3) + hidden_biases_3), keep_prob_3)\n",
        "  \n",
        "  # output layer\n",
        "  output_weights = tf.Variable(\n",
        "    tf.truncated_normal(\n",
        "      [hidden_nodes_3, num_labels], stddev=output_stddev))#\n",
        "  output_biases = tf.Variable(tf.zeros([num_labels]))\n",
        "  \n",
        "  \n",
        "  # Training computation.\n",
        "  \n",
        "  logits = tf.matmul(hidden_3, output_weights) + output_biases  \n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
        " \n",
        "  loss += regu * (tf.nn.l2_loss(hidden_weights_1) +\n",
        "                  tf.nn.l2_loss(hidden_weights_2) +\n",
        "                  tf.nn.l2_loss(hidden_weights_3) +\n",
        "                  tf.nn.l2_loss(output_weights))\n",
        "\n",
        "  \n",
        "  \n",
        "  # Learn with exponential rate decay.\n",
        "  global_step = tf.Variable(0, trainable=False)\n",
        "  starter_learning_rate = 0.5\n",
        "  learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 100000, 0.95, staircase=True)\n",
        "  \n",
        "  # Optimizer.\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
        "  train_prediction = tf.nn.softmax(logits)\n",
        "  \n",
        "\n",
        "  \n",
        "  # Predictions for the training, validation, and test data. \n",
        "  # Validation prediction step.\n",
        "  validation_hidden_1 = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights_1) + hidden_biases_1)\n",
        "  validation_hidden_2 = tf.nn.relu(tf.matmul(validation_hidden_1, hidden_weights_2) + hidden_biases_2)\n",
        "  validation_hidden_3 = tf.nn.relu(tf.matmul(validation_hidden_2, hidden_weights_3) + hidden_biases_3)\n",
        "  validation_logits = tf.matmul(validation_hidden_3, output_weights) + output_biases\n",
        "  validation_prediction = tf.nn.softmax(validation_logits)\n",
        "\n",
        "  # Test prediction step.  \n",
        "  test_hidden_1 = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights_1) + hidden_biases_1)\n",
        "  test_hidden_2 = tf.nn.relu(tf.matmul(test_hidden_1, hidden_weights_2)+ hidden_biases_2)\n",
        "  test_hidden_3 = tf.nn.relu(tf.matmul(test_hidden_2, hidden_weights_3) + hidden_biases_3)\n",
        "  test_logits = tf.matmul(test_hidden_3, output_weights) + output_biases\n",
        "  test_prediction = tf.nn.softmax(test_logits)\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AB2cqvdEnLEs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "e06381a9-6546-4a44-c4af-d62bda0f0fad"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 20001\n",
        "\n",
        "with tf.Session(graph=deep_graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "  print(\"Initialized\")\n",
        "  for step in range(num_steps):\n",
        "    # Pick an offset within the training data, which has been randomized.\n",
        "    # Note: we could use better randomization across epochs.\n",
        "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
        "    # Generate a minibatch.\n",
        "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
        "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
        "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
        "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
        "    # and the value is the numpy array to feed to it.\n",
        "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "    _, l, predictions = session.run(\n",
        "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
        "    if (step % 500 == 0):\n",
        "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
        "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
        "      print(\"Validation accuracy: %.1f%%\" % accuracy( validation_prediction.eval(), valid_labels))\n",
        "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 3.914595\n",
            "Minibatch accuracy: 9.4%\n",
            "Validation accuracy: 12.0%\n",
            "Minibatch loss at step 500: 1.609692\n",
            "Minibatch accuracy: 79.7%\n",
            "Validation accuracy: 84.5%\n",
            "Minibatch loss at step 1000: 1.150748\n",
            "Minibatch accuracy: 83.6%\n",
            "Validation accuracy: 85.8%\n",
            "Minibatch loss at step 1500: 0.831414\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 86.6%\n",
            "Minibatch loss at step 2000: 0.808347\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 86.6%\n",
            "Minibatch loss at step 2500: 0.574893\n",
            "Minibatch accuracy: 89.8%\n",
            "Validation accuracy: 86.8%\n",
            "Minibatch loss at step 3000: 0.736293\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 87.1%\n",
            "Minibatch loss at step 3500: 0.636759\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 86.8%\n",
            "Minibatch loss at step 4000: 0.549261\n",
            "Minibatch accuracy: 91.4%\n",
            "Validation accuracy: 87.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XFKv415x-5XG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Test accuracy: 92.6%\n",
        "\n"
      ]
    }
  ]
}